---
title: "Tutorial Causal Trees DSE 2020 Tilburg"
author: Madina Kurmangaliyeva
output: learnr::tutorial
tutorial:
  id: "tutorial_causal_trees"
runtime: shiny_prerendered
description: "Causal Trees and Causal Forests"
---
  
```{r setup, include=FALSE}
library(learnr)
library(DiagrammeR)
library(grf)
library(tidyverse)

knitr::opts_chunk$set(echo = FALSE)

# getting COMPAS data

set.seed(111)
dataset <- fairness::compas %>% 
  filter(ethnicity == c("Caucasian", "African_American")) %>%
  mutate(ethnicity = as.factor(as.character(ethnicity))) %>%
  mutate(Number_of_Priors = Number_of_Priors - min(Number_of_Priors)) %>% 
  select(-probability,-predicted, -Two_yr_Recidivism) %>%
  sample_n(1000)

# Simulating data
dataset <- dataset %>% mutate(
  jobhelp = sample(c(0,1), 1000, replace = TRUE, prob = c(0.5, 0.5)),
  )

datamat <- dataset %>%  model.matrix(~., .) %>% as.data.frame() %>% 
  select(-`(Intercept)`) %>% 
  mutate(
    has_priors = if_else(Number_of_Priors > 0, 1, 0),
    wage_nohelp = (-0.15)*has_priors + 
            (-0.1)*Age_Above_FourtyFiveyes + (-0.1)*Age_Below_TwentyFiveyes  +
            0.1*ethnicityCaucasian + 
            (-0.05)*has_priors*ethnicityCaucasian +
            rlnorm(1000, meanlog = 0, sdlog = 0.1),
    help_effect = 0.05*Number_of_Priors,
    wage = wage_nohelp + help_effect*jobhelp
    )

# ggplot(aes(x = wage, color = as.factor(jobhelp)), data = datamat) + geom_density()
# ggplot(aes(x = Number_of_Priors, y = help_effect), data = datamat) + geom_point()

dataset <- dataset %>% 
  mutate(wage = datamat$wage)

X <- dataset %>%  select(-wage, -jobhelp) %>%  
  model.matrix(~., .) 
X <- X[ , -1]
Y <- dataset$wage
W <- dataset$jobhelp

set.seed(2345)
cforest <- causal_forest(
  X,
  Y,
  W,
  num.trees = 4000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 10,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 200,
  tune.num.reps = 50,
  tune.num.draws = 1000,
  compute.oob.predictions = TRUE
)


npriors_val <- unique(dataset$Number_of_Priors)
Xtest <- matrix(0, length(npriors_val), ncol(X))
Xtest[,1] <- npriors_val

set.seed(3456)
cf_predict <- predict(cforest, Xtest, estimate.variance = TRUE) %>%
  mutate(Number_of_Priors = Xtest[ ,1])


```



## Intro: Hypothetical RCT

In this tutorial, we imagine a following situation. 

Assuming that the most important predictor of criminal behavior is poverty and lack of employment opportunities, the U.S. government mandated  a new job counselling and job placement  program for ex-offenders. Since the program was costly, the government decided to test its effectiveness by first running an RCT. 


Imagine that half of the offenders on the COMPAS list were mandated to go through comprehensive job counselling and help with job placement, while the other half did not have any access to this program. At this moment we are abstracting from the recidivism rates.  

I modified the COMPAS data you used in the previous tutorials in the following manner:

* Dropped `Two_yr_Recidivism` variable
* Added `jobhelp` dummy variable that indicates whether the individual was assigned to treatment or not
* Added `wage` variable which records the average of the monthly income (in thousand USD) during the three years after the end of the job placement program for both treated and non-treated individuals.
* Shifted `Number_of_Priors` so that it starts with zero.

Both `jobhelp` and `wage` variables are simulated.

You are part of the program assessment team and your task is to find the following information about the treatment effect of `jobhelp` on `wage`:

* Average Treatment Effect (ATE)
* Average Treatment Effect on the Treated
* Conditional ATE for white offenders
* Conditional ATE for black offenders
* Find which group of offenders will benefit the most from the program
* Find which group of offenders does not benefit and do not need the program 
* Which characteristics of the offenders define the most the heterogeneity of treatment effects
* Predict treatment effects for any new offender

In this tutorial we will learn how to answer these questions using the Causal Forest algorithm.



### Exercise -- Get acquainted with the dataset.


*Complete the code below by illing in the blanks*

```{r dataintro, exercise=TRUE, exercise.eval=FALSE}
# See the head of the data
____(dataset)

# Get the density plot of the variable 'wage'
dataset %>% ggplot(...) + 
  
# Get the number of people who were treated
dataset %>% 
  

```

```{r dataintro-hint}
# See the head of the data
head(dataset)

# Get the density plot of wages
dataset %>% ggplot(aes(x = wage)) + geom_density()
  
# Get the number of people who were treated
dataset %>% count(jobhelp)

```




```{r quiz_howmany}
quiz(
  question("How many people were treated with the mandatory job placement help?",
    answer("1000"),
    answer("499"),
    answer("501", correct = TRUE)
  )
)
```


## 1. Growing a Causal Forest

```{r quiz_ct}
quiz(
  question("Causal Forest (CF) is different from Random Forest because",
    answer("CF does not randomly sample observations when growing trees"),
    answer("CF does not randomly sample variables when considering how to split data"),
    answer("CF grows causal trees instead of decision trees ", correct = TRUE),
    answer("CF's objective is to capture differences in treatment effects rather than in the outcome", correct = TRUE)
  )
)
```

```{r quiz_ct2}
quiz(
  question("In comparison to a Decision Tree, growing a Causal Tree:",
    answer("requires more observations because CT uses just half of the data due to Honest Target", correct = TRUE),
    answer("allows to get most accurate prediction of the outcome"),
    answer("allows to get a consistent estimate of treatment effects", correct = TRUE),
    answer("requires a random assignment of treatment", correct = TRUE)
  )
)
```


Since the assignment to the job placement program was completely random, and there was full compliance (mandatory program for ex-offenders), we can grow a Causal Forest to estimate the treatment effects.


To grow a Causal Forest we will use the `causal_forest()` function from `grf` package.


### Exercise -- Convert the dataset to a matrix 

Unfortunately, `causal_forest()` accepts data only in matrix or vector form, not a data.frame object. Hence, the first step is to create the following objects: 

* matrix `X` of predictors
* vector `Y` which contains wage information
* vector `W` which contains treatment status.

*Fill in the blanks below, put the correct formula instead of the question mark. Hint: use `model.matrix()` function automatically convert a dataset into a numeric matrix*

```{r createvars, exercise=TRUE, exercise.eval=FALSE}
Y <- dataset$____
W <- dataset$____

X <- dataset %>%  select(-____, -____) %>%  
  _____.______( ~ ?, data = .)

# Drop the intercept (No need to change anything here)
X <- X[ , -1]
```

```{r createvars-hint}
Y <- dataset$wage
W <- dataset$jobhelp

X <- dataset %>%  select(-wage, -jobhelp) %>%  
  model.matrix(~., .) 
X <- X[ , -1]

```


### Exercise -- Grow a Causal Forest

Now we are ready to grow our first Causal Forest.

Please, grow a causal forest with the following parameters: 

* 4000 causal trees.
* We set `clusters = NULL` since out treatment was fully randomized, and not randomized within clusters 
* For each individual causal tree use only a random 50% of observations (i.e., specify so for `sample.fraction`). 
* Use all of the variables as potential split candidates by putting the correct `mtry` (you can always try with fewer variables later). 
* Require the node size to be ten or more observations using  `min.node.size`. 
* Keep 50% of the sampled data as estimation sample, and 50% as training sample using `honesty.fraction`. (Notice that it means that for any given tree we use 25% of initial data either to train or to estimate since we sample only half of data to begin with)
* We set `honesty.prune.leaves = TRUE` in order to trim leaves that end up empty in the estimation sample
* We set `tune.parameters = "none"`, since we are skipping cross-validation part for the sake of time. Otherwise, "the following parameters are tunable: ("sample.fraction", "mtry", "min.node.size", "honesty.fraction", "honesty.prune.leaves", "alpha", "imbalance.penalty")"

Save the resulting forest as `cforest`.

*Please,fill in the gaps in the formula below*
```{r cf, exercise=TRUE, exercise.eval=FALSE }
set.seed(2345)
cforest <- causal_forest(
  X = _,
  Y = _,
  W = _,
  num.trees = ____,
  clusters = NULL,
  sample.fraction = __,
  mtry = ____,
  min.node.size = __,
  honesty = TRUE,
  honesty.fraction = __,
  honesty.prune.leaves = TRUE,
  tune.parameters = "none"
)
```

```{r cf-hint}
set.seed(2345)
cforest <- causal_forest(
  X = X,
  Y = Y,
  W = W,
  num.trees = 4000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = ncol(X),
  min.node.size = 10,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  tune.parameters = "none"
)
```



## 2. Variable importance

### Exercise -- Get a plot of a single tree

We have successfully trained a Causal Forest and saved it as `cforest`.  

We know that one of the downsides of forests is that we can no longer represent the segmentation graphically as a tree, because a forest combines information from many trees.

Nevertheless, we can still access one of the 4000 causal trees we grew as part of the forest and visualize it.

For example, the code below uses `get_tree()` function to access the very first causal tree (out of 4000) and plots it. The resulting plot tells you about the splitting rules, the number of observations in each resulting leaf using estimation sample, and the mean outcome "avg_Y" (unfortunately, not the mean treatment effect) and the share of treated "avg_W".


*Change the code below to access the 1234th causal tree*

```{r plotonetree, exercise=TRUE, exercise.eval=TRUE}
plot(get_tree(cforest, index=1))
```

```{r plotonetree-hint}
plot(get_tree(cforest, index=1234))
```

Remember that the very first split  of a decision tree usually indicates the most important predictor for Y. In the end, the very first split finds the variable that helps the most in minimizing RSS. Similarly, a causal tree first splits at a variable that helps the most to capture differences in treatment effects.

```{r quiz_t1}
quiz(
  question("The most important variable that affects heterogeneity in treatment effects in causal tree number 1 is:",
    answer("Number_of_Priors", correct = TRUE),
    answer("Ethnicity"),
    answer("Female"),
    answer("Misdemeanor")
  )
)
```

```{r quiz_t1234}
quiz(
  question("The most important variable that affects heterogeneity in treatment effects in causal tree number 1234 is:",
    answer("Number_of_Priors"),
    answer("Ethnicity"),
    answer("Female", correct = TRUE),
    answer("Misdemeanor")
  )
)
```

As you can see the split rules changed quite a lot if we compare the very first tree to the 1234th tree. This instability is remedied exactly by bootstrapping our sample 4000 times.

A better way to see the importance of the variable is by checking how often any given variable was used for splitting overall across all 4000 trees. 

### Exercise -- Find which variable is most important for predicting heterogeneity of treatment effects

We can use function `variable_importance()` to get a simple weighted sum of how many times variable $i$ was split on at each depth in the forest. Hence, the most important variable for the heterogeneity in treatment effects will have the highest share of splits.



```{r importance, exercise=TRUE, exercise.eval=FALSE}

variable_importance(_____)

```

```{r importance-hint}

variable_importance(cforest)

```

```{r quiz_imp}
quiz(
  question("The most important variable that affects heterogeneity in treatment effects is:",
    answer("The first predictor -- `Number_of_Priors` -- with 73.57% of weighted splits attributable to this variable", correct = TRUE),
    answer("The second predictor -- `Age_Above_FortyFiveyes`  -- with (100 - 3.65)% of weighted splits  attributable to this variable"),
    answer("There is no clear most important predictor of heterogeneity")
  )
)
```


## 3. Estimating Treatment Effects

Finally, let's calculate some treatment effects.

Function `average_treatment_effect()` does exactly what we want.

### Exercise -- Calculate ATE and ATT

For example, in the exercise code below you can already see the example of how to calculate ATE. Since we are interested in ATE, we indicate that the target sample is "all".

*Change the target sample to "treated" to get the ATT*

```{r ate, exercise = TRUE, exercise.eval = TRUE}
set.seed(456)
average_treatment_effect(cforest, target.sample = "all")
```

```{r ate-hint}
set.seed(456)
average_treatment_effect(cforest, target.sample = "treated")
```

```{r quiz_ate}
quiz(
  question("What does an ATE of 0.035 mean for the average monthly income of ex-offenders in the next three years?",
    answer("The mandatory job assistance program increases on average the monthly income of the treated by 3.5%."),
    answer("The mandatory job assistance program increases on average the monthly income  of the treated by 35 USD", correct = TRUE),
    answer("The mandatory job assistance program increases the average monthly income growth rate  of the treated by 3.5 percentage points"),
    answer("The mandatory job assistance program explains 3.5% of the average monthly income gap between the treated and the control group")
  )
)
```

```{r quiz_ateatt}
quiz(
  question("The ATE is not significantly different from ATT. Why is it so?",
    answer("It just a coincidence"),
    answer("We expected it to be so", correct = TRUE),
    answer("Thanks to the randomized treatment, treated and controls have on average the same treatment effects, i.e., the groups are truly balanced", correct = TRUE)
  )
)
```




### Exercise -- Calculate ATE for black and white ex-offenders separately

We can also investigate how the ATE changes within different subgroups of ex-offenders.

For example, the code below calculate the Conditional ATE for Caucasian ex-offenders.

*Change the code below to calculate ATE for black ex-offenders. Hint: you just need to change one character; remember that we have just two ethnicity groups in the sample: Caucasians and African-Americans*

```{r cate_ethnicity, exercise = TRUE, exercise.eval = TRUE}
average_treatment_effect(cforest, target.sample = "all", subset = X[ , "ethnicityCaucasian"] == 1)
```

```{r cate_ethnicity-hint}
average_treatment_effect(cforest, target.sample = "all", subset = X[ , "ethnicityCaucasian"] == 0)
```

```{r quiz_ethnate}
quiz(
  question("As you can see ATE for African-American ex-offenders is twice higher than the ATE for Caucasian ex-offenders. Combining this result with  the results of the previous section which revealed the variables that are important for the heterogeneity in TE, we can surmise that:",
    answer("The ethnicity is in fact one of the most important factors determining the heterogeneity in TE"),
    answer("Black and white ex-offenders probably on average differ in the number of priors -- the most important factor determining the heterogeneity in TE --  resulting that their ATE to be different too.", correct = TRUE),
    answer("There is no clear hypothesis for such difference")
  )
)
```

### Exploring heterogeneity in treatment effects

Remember that the `variable_importance()` function in the previous section indicated that by far `Number_of_Priors` was the most important factor explaining the heterogeneity in treatment effects.

Hence, now we want to explore how treatment effects differ at different values of `Number_of_Priors`. How can we do that?

We can create a new matrix X, where we fix all other variables but we vary only the values of `Number_of_Priors`.

For example, we can create a matrix, `X_test`, with all variables set to zero. First, recognize that `Number_of_Priors` is in fact a discrete variable. In the code below we first capture all the unique values `Number_of_Priors` has in our dataset and save it as `npriors_val`. Then we create the matrix with all variables set to zero, except the first variable which corresponds to `Number_of_Priors`. Here, we substitute it with the grid of values saved in `npriors_val`. Finally, the code shows you the head of the matrix.

```{r createXtest, echo = TRUE}

npriors_val <- unique(dataset$Number_of_Priors)
Xtest <- matrix(0, length(npriors_val), ncol(X))
Xtest[,1] <- npriors_val
head(Xtest)
```



### Exercise -- Predict treatment effects for the artificial test data

Now, since we have constructed our `Xtest` data where we control for all other predictors but vary the number of priors, we can create personalized estimates of the treatment effects. We use function `predict()` to get the TE predictions for the hypothetical new observations -- each row of the `Xtest` -- using the model saved as `cforest`.  Notice that we also ask to estimate the variance of the TE estimator. Save the prediction as `cf_predict` dataset.


*Finish the code below, substitute blanks*

```{r predict, exercise = TRUE, exercise.eval = FALSE}
set.seed(3456)
cf_predict <- predict(object = _______, 
                      newdata = ____, 
                      estimate.variance = TRUE) %>%
  mutate(Number_of_Priors = Xtest[ ,1])

head(cf_predict)
```

```{r predict-hint}
set.seed(3456)
cf_predict <- predict(object = cforest, 
                      newdata = Xtest, 
                      estimate.variance = TRUE) %>%
  mutate(Number_of_Priors = Xtest[ ,1])

head(cf_predict)
```


As you can see it creates a dataframe with two variables: 

1. predictions -- stores the estimated Treatment Effect at given values of X
2. variance.estimates -- stores the variance of the TE estimator at given values of X

### Exercise -- Visualize the heterogeneity

Finally, visualize the estimated heterogeneity in treatment effects due to `Number_of_priors`. You want  a graph which shows how the estimated Treatment Effects (y-axis) vary depending on the number of priors (x-axis). We also want 95% confidence interval. Remember, that we can get the 95% confidence interval by adding or subtracting 1.96*SE of the estimator.


*Fill in the blanks in the code below*

```{r predictgraph, exercise = TRUE, exercise.eval = FALSE}
ggplot(aes(x = ________, y = ______), data = ______) +
  geom_errorbar(aes(ymin=predictions - 1.96*sqrt(________),
                    ymax=predictions + 1.96*sqrt(________)), colour = "black", width=.1) +
    geom_line() +
    geom_point(size=3)
```

```{r predictgraph-hint}
ggplot(aes(x = Number_of_Priors, y = predictions), data = cf_predict) +
  geom_errorbar(aes(ymin=predictions - 1.96*sqrt(variance.estimates),
                    ymax=predictions + 1.96*sqrt(variance.estimates)), colour = "black", width=.1) +
    geom_line() +
    geom_point(size=3)
    
```

```{r quiz_graph}
quiz(
  question("What does the graph say?",
    answer("The program is more beneficial for people with fewer priors"),
    answer("The program is more beneficial for people with more priors", correct = TRUE)
  )
)
```

```{r quiz_graph2}
quiz(
  question("Why do you think the estimated TE increase with the number of priors in sort of linear fashion but then become flat for higher number of priors?",
    answer("It could be because the positive effect of the treatment indeed stops growing after a certain number of priors", correct = TRUE),
    answer("It could be that the positive effect of the treatment keeps growing but we are unable to capture the heterogeneity there due to a limited sample size", correct = TRUE)
  )
)
```

To see whether indeed we have too few observations in the right tail of the distribution of `Number_of_Priors`, we can plot the distribution:

```{r distr_priors, echo = TRUE}
dataset %>% ggplot(aes(x = Number_of_Priors)) + geom_density()
```


Indeed, the graph shows that we have too few observations for the values greater than 1.5. So we actually cannot know for sure whether the flattening is caused by true change in the relationship between treatment effects and the number of priors,  or due to insufficient data in that region. 




### Little secret

Wait, actually we can. Well, not in reality, not when you work with real data. But this time, I simulated the data, I know the true treatment effects. 

The true formula for treatment effects was `te = 0.05*Number_of_Priors`. Hence, let's plot everything again with the true values.

```{r plotagain, echo = TRUE}
cf_predict_withtruth <- cf_predict %>%  
  mutate(true_effect = 0.05*Xtest[ ,1])

ggplot(aes(x = Number_of_Priors, y = predictions), data = cf_predict) +
  geom_errorbar(aes(ymin=predictions - 1.96*sqrt(variance.estimates),
                    ymax=predictions + 1.96*sqrt(variance.estimates)), colour = "black", width=.1) +
    geom_line() +
    geom_point(size = 3) +
    geom_line(aes(x = Number_of_Priors, y = true_effect ), data = cf_predict_withtruth, color = "red")
```


### So what happened?

Now you can clearly see that the Causal Forest did not have enough data to split on `Number_of_Priors` for values beyond 1.5. So it lumped together those values and estimated the average treatment effect for a group of offenders with number of priors greater than 1.5-2.0.

However, this is how Causal Forest works for extreme values. In fact, it did a pretty good job for more frequent groups of offenders with the  number of priors in the range of 0 to 1.5.

Now, since you know that I created the heterogeneous treatment effects using only `Number_of_Priors` as a mediator, take a second to appreciate that the Causal Forest indeed did find that `Number_of_Priors` is  the most (and by far only) important factor capturing the heterogeneity in treatment effects. 

You did not have to write a single line of sophisticated formula in a regression to uncover. All you had to do is to pass X, Y, and W objects to `causal_forest()` function. That's what makes this tool amazing. Now the search for heterogeneity in treatment effects is truly data-driven.



## Afterword

Now, creating an artificial Xtest matrix and varying only one variable at a time may help you uncover the extent of heterogeneity across different dimensions at an analytical level.

However, you still need to deliver guidance to the government, right?


You can use the results of the causal forest that creates prediction of the TE for every new ex-offender, compares the expected TE to the marginal cost of the job assistance program,  and then it delivers personalized recommendations of treatment eligibility (yes/no) for that particular ex-offender.

Alternatively, you can try to grow just one causal tree and use the segmentation delivered by that tree to tell policymakers which groups are eligible and which are not. However, as we saw that using just one tree might be a bad idea since individual trees might be unstable (non-robust). In this case, you need to use cross-validation  for every single free parameter in order to prune the tree in a correct manner. In general, you can then see how worse your single Causal Tree performs with respect to the Causal Forest. If the performance is comparable, then you may use a single Causal Tree to inform policymakers, as this approach is much more transparent.

In either case, be prepared to be attacked on the fairness grounds. You would need to check in advance that your policy prescriptions would be seen as fair. Also, you need to defend: 

* why you use wage as the only  outcome. Are there other important outcomes that we need to capture the welfare effect of the program? 
* Have you used enough observations? 
* and so on, many other questions, be prepared...





